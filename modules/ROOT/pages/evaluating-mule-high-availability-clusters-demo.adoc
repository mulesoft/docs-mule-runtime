= Configuring Mule High Availability Clusters Demo
ifndef::env-site,env-github[]
include::_attributes.adoc[]
endif::[]
:keywords: deploy, clusters, high availability, demo

*_Enterprise_*

Mule Enterprise Edition supports scalable *clustering* to provide *high availability (HA)* for applications.

You can cluster two or more Mule instances together in an *active-active* model to manage http://en.wikipedia.org/wiki/Failover[failover] and ensure reliability. Because they are built on Mule’s in-memory data grid, clustered Mule instances can run the same applications, and actively share the workload between them. If one instance, or *node*, fails, the others seamlessly pick up the load without interrupting service.

[TIP]
You can now easily create a cluster through the Runtime Manager UI. See xref:runtime-manager::managing-servers.adoc#create-a-cluster[Creating and Managing Clusters].

*Demo Starts Here*: xref:1-installing-the-demo-bundle.adoc[1 - Installing the Demo Bundle]


== What is High Availability?

[NOTE]
====
*High Availability* is a method of designing a computer system to prevent any downtime for the applications that run on it. Some systems use multiple servers so that if one server experiences downtime, the application can continue to run smoothly on the others, without interrupting service for the application’s end users.
====

== Cluster Design and Management

The *Mule Management Console* and *Runtime Manager* are two interfaces that enable you to set up a cluster of Mule instances, then deploy an application to run on the cluster. You can also monitor the status information for clusters and individual nodes. When clustered, you can easily manage several servers as one.

[NOTE]
The most current information on cluster management for Mule runtime 3.8 and later is in the xref:runtime-manager::managing-servers.adoc[Managing Servers] section of the xref:runtime-manager::index.adoc[Runtime Manager]

A Mule *Cluster* consists of 2 - 8 Mule server instances, or *nodes*, grouped together and treated as a single unit. Thus, you can deploy, monitor, or stop all the nodes in a cluster as if they were a single Mule server. All the nodes in a cluster share memory, as illustrated below:

image::topology-4-cluster.png[topology_4-cluster]

Mule uses an *active-active* model to cluster servers, rather than an *active-passive* model.

In an *active-passive* model, one server in a cluster acts as the *primary*, or active node, while the others are *secondary*, or passive nodes. The application in such a model runs on the primary server, and only ever runs on the secondary server if the first one fails. In this model, the processing power of the secondary node(s) is mostly wasted in passive waiting for the primary node to fail.

In an *active-active* model, no one server in the cluster acts as the primary server; all servers in the cluster support the application. This application in this model runs on all the servers, even splitting apart message processing between nodes to expedite processing across nodes.

*Learn more*:  xref:mule-high-availability-ha-clusters.adoc[Mule High Availability (HA) Cluster Overview]

== Benefits of Clustering

*High Availability*: If one node fails, outstanding tasks transfer automatically to the surviving node(s), which continue to process messages, ensuring uninterrupted service.

*Throughput*: The nodes in a cluster work in parallel so that each processes a different message (or performs s different operation on the same message). Mule uses VM queues to automatically balance the load between nodes, further optimizing throughput. By replicating the queues across the cluster, Mule ensures that the first available node processes a message, and messages never get lost.

*Resource Coordination*: Mule automatically coordinates access to each resource (such as a file server or a database table) to prevent utilization conflicts.

*Scalability*: Add or subtract Mule instances from your cluster to efficiently manage increases or decreases in load. You can also scale each instance _internally_ to maximize its processing efficiency. In other words, a single Mule instance can take advantage of the increased processing power offered by multiple cores servicing this newly enhanced Mule instance, even though that instance continues to act as a single node within the parent cluster.

== Using the Demo to Explore HA

To see Mule HA clusters in action, download and install the *Mule HA Demo Bundle*. This free, hands-on demo helps you to quickly learn and evaluate Mule HA first-hand.

=== What You Learn

Designed to help new users evaluate the reliability of Mule High Availability Clusters, the *Mule HA Demo Bundle* teaches you how to use the Mule Management Console to create a cluster of Mule instances, then deploy an application to run on the cluster. Further, this demo simulates two processing scenarios that illustrate the cluster’s ability to automatically balance normal processing load, and its ability to reliably remain active in a failover situation.

*What's Not In This Document* +
The *Mule HA Demo Bundle* demonstrates the power, agility and reliability of using a Mule cluster. Within the context of this example, the Mule cluster _does not_ demonstrate its ability to improve processing performance. That is a demo for another day!

=== Included in the Demo Bundle

The *Mule HA Demo Bundle* includes several items which enable you to examine a functional Mule cluster.

. *A Mule bundled distribution* which includes:

* Mule Enterprise standalone

* Mule Management Console
. *A demo application to deploy on the cluster.* +
 This *cluster-demo-app* consists of four xref:mule-application-architecture.adoc[flows]. The application receives messages from a JMS queue, adds information to the messages, then dispatches them to another JMS queue. It uses two types of transactions to safely process messages:

* http://en.wikipedia.org/wiki/X/Open_XA[XA] between JMS and VM queues

* local between VM endpoints +
 We use a high availability cluster to run this demo because the application conducts transactions; an HA cluster guarantees that messages don’t get lost when a node becomes unavailable.

. *A Web application, `WidgetUI`, to apply load to the Mule cluster.* +
 The Web application simulates calls to the *cluster-demo-app* that runs on the cluster, thereby enabling you to witness cluster reliability. When the `WidgetUI` Web app applies load to the cluster, you can examine statistics that illustrate how the Mule cluster balances load when processing messages.
+
image::widgetui-modified.png[widgetUI_modified]

== Get Started

xref:1-installing-the-demo-bundle.adoc[1 - Installing the Demo Bundle]

xref:2-creating-a-cluster.adoc[2 - Creating a Cluster]

xref:3-deploying-an-application.adoc[3 - Deploying an Application]

xref:4-applying-load-to-the-cluster.adoc[4 - Applying Load to the Cluster]

xref:5-witnessing-failover.adoc[5 - Witnessing Failover]

xref:6-troubleshooting-and-next-steps.adoc[6 - Troubleshooting and Next Steps]